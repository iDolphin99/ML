{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Week_10-2_PyTorch_ANN.ipynb","private_outputs":true,"provenance":[{"file_id":"13Kr44TUrLrmJq-M6IOTC4fAuks4qWZmT","timestamp":1635932296268}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"VskMijOr0wOe"},"source":["# ANN, MLP과 관련된 내용들을 다뤘음\n","# ANN이 training 되는 과정에서 Gradient Descent, Backpropagation이 작동됨 \n","# 추가로 ANN을 효과적으로 활용하기 위해서 Regularization, Optimization 방법이 있음\n","\n","import numpy as np\n","import torch\n","from torch import nn             # nn-> 신경망을 활용하기 위해 필요한 module\n","import matplotlib.pyplot as plt  # logistic regression을 시각화하기 위함 \n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-BohvQ4k1BE"},"source":["## Logistic Regression (PyTorch)"]},{"cell_type":"code","metadata":{"id":"UOEM8HG9200q"},"source":["# 데이터를 생성해서 점들로 표현한 후, 점들을 classification 하는 문제\n","# 1000개씩 2줄 형태로 data setting \n","# X0와 X1을 구분해서 생성 -> 분류를 위함 \n","# X0 -> normal distribution, random하게 정의, 평균이 2가 됨 \n","# X1 -> 평균이 -2가 되는 data set \n","# y0 -> 0, y1 -> 1 \n","\n","n_data = torch.ones(1000, 2)\n","\n","X0 = torch.normal(2 * n_data, 1)\n","y0 = torch.zeros(1000)\n","X1 = torch.normal(-2 * n_data, 1)\n","y1 = torch.ones(1000)\n","\n","\n","# vstack을 활용하여 X,y끼리 묶어줌 \n","# train_y가 true/false 인 case로 정의 \n","train_X = np.vstack([X0, X1])\n","train_y = np.vstack([y0, y1]).reshape(-1, 1)\n","\n","C1 = np.where(train_y == True)[0]\n","C0 = np.where(train_y == False)[0]\n","\n","\n","# Define train_X, train_y for torch\n","# torch를 활용한 training을 위해 이에 맞는 형태로 data define \n","# from_numpy() -> numpy로부터 data를 받음, X data가 normal distribution을 따르는 random한 숫자들을 포함하므로 float형태로 나옴\n","# 2000개의 data 중 1000개는 true, 1000개는 false와 관련된 data\n","train_X, train_y = torch .from_numpy(train_X).float(), torch.from_numpy(train_y).float() \n","\n","print(train_X.shape, train_y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4XNzmNfu_3L"},"source":["# data plot \n","# C0 -> x1, x2 도 평균이 2정도, 표준편차가 1이 되는 data들\n","# C1 -> x1, x2 모두 -2를 평균으로 갖는, normal distribution을 따르는 data들 \n","# C1, C2를 하나의 hyper plane을 통해서 두 개의 data를 classification하는 것 \n","\n","plt.figure(figsize = (10,8))\n","plt.plot(train_X[C1,0], train_X[C1,1], 'ro', alpha = 0.3, label='C1')\n","plt.plot(train_X[C0,0], train_X[C0,1], 'bo', alpha = 0.3, label='C0')\n","plt.xlabel(r'$x_1$', fontsize = 15)\n","plt.ylabel(r'$x_2$', fontsize = 15)\n","plt.legend(loc = 1, fontsize = 12)\n","plt.axis('equal')\n","plt.ylim([-5,5])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmEw9Vk5mdv6"},"source":["# modeling에 적용하기 위해서는 dataloader, tensordataset을 활용해주어야 함 \n","# TensorDataset : 학습하고자 하는 data들, tensor들을 찾는 과정 -> 일반적으로 x, y data가 많이 포함됨 \n","# 우리는 x,y를 data array 형태로 받음 \n","# dataset을 data array형태로 TensorDataset을 받아옴 \n","# dataloader는 어떤 dataset을 쓰고, batch size, shuffle 어떻게 되는지 포함하는 function \n","\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def load_array(data_arrays, batch_size, is_train = True):\n","    # Define dataset and dataloader\n","    dataset = TensorDataset(*data_arrays)\n","    dataloader = DataLoader(dataset = dataset, \n","                            batch_size = batch_size,\n","                            shuffle = is_train)\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0u13o66meWm"},"source":["# train_X, train_y를 batch size는 나누지 않고 전체 데이터를 통으로 data iteration을 적용\n","# 학습을 하기 위한 형태로 데이터를 정리함 \n","\n","data_iter = load_array((train_X, train_y), batch_size=len(train_y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCjW11cUncIX"},"source":["# torch.nn Module class를 상속받아 Logistic Regression Mdel class를 사용할 수 있게 됨\n","# 새로운 DL model을 설계하는데 효과적으로 활용될 수 있음 \n","# init : Logistic rigression model class의 instance를 생성했을 때 갖게 되는 성질 정의\n","\n","\n","class LogisticRegressionModel(torch.nn.Module):\n","    # Define init, forward method\n","    # NN model 내 method를 상속받게 함\n","    # layer는 처음에 Linear, input later ~ output layer 뉴런의 수*노드의 수) 입력 \n","    # X1 X2를 받아서 y를 맞춰야 하므로 2, 1\n","    # linear로 결합된 값을 sigmoid function을 통해서 최종적으로 결론이 난다 \n","    def __init__(self):\n","        super(LogisticRegressionModel, self).__init__()  \n","        self.layer = torch.nn.Linear(2, 1)              \n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    # 하나 하나의 layer들의 관계를 정의해 줌 \n","    # 도출되는 output은 self.layer의 input을 넣었을 때가 됨 \n","    # 첫 번째 layer를 통과한 output은 sigmoid function을 통과시킴 \n","    def forward(self, inputs):\n","        outputs = self.layer(inputs)\n","        return self.sigmoid(outputs)\n","\n","model_logR = LogisticRegressionModel()\n","\n","# cuda, GPU를 추가적으로 사용할지 묻는 코드 \n","# GPU를 사용한다면 train_X, train_y, model까지 GPU를 사용할 수 있도록 정의해주어야 함\n","#if torch.cuda.is_available():\n","#    train_X, train_y = train_X.cuda(), train_y.cuda()\n","#model_logR.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3RkCQRXoYGl"},"source":["# model log 아래 layer들의 weight와 bias를 initialize한다 \n","\n","print(model_logR.layer.weight.data)\n","print(model_logR.layer.bias.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlWzNa1Rodve"},"source":["# Define SGD optimizer \n","# torch의 optimizer를 나타내는 module 밑의 SGD function을 이용\n","\n","optimizer_logR = torch.optim.SGD(model_logR.parameters(), lr = 0.05)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCH8SooSlLjS"},"source":["# parameters : layer와 어떤 activation function을 사용하는지 나옴 \n","\n","model_logR.parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUvM68hHolNf"},"source":["# Training 단계\n","\n","num_epochs = 200\n","loss_graph_logR = []\n","\n","# Train X data를 model에 넣고 binary cross entropy를 통해 loss를 계산한 후 \n","# 최적화에 대해서 gradient를 초기화한 후 loss에 대해 backward을 진행, optimizer에 대해서 관련된 weight들을 update한다\n","# X를 넣어서 y를 도출하고 model을 통해 도출된 y와 train_y를 비교해서 loss를 구한 후 \n","# gradient를 초기화, backpropagation, update하는 순서로 진행함 \n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        # training with predict, loss, zero_grad, backward, step\n","        predict_logR = model_logR(train_X)\n","        loss_logR = torch.nn.functional.binary_cross_entropy(predict_logR, train_y)\n","        optimizer_logR.zero_grad()\n","        loss_logR.backward()\n","        optimizer_logR.step()\n","    loss_graph_logR.append(torch.nn.functional.binary_cross_entropy(model_logR(train_X), train_y))\n","\n","plt.plot(loss_graph_logR)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zHOMYlxv_oO"},"source":["# logistic regression model에 대한 weight를 구함 -> x1, x2에 대한 weight \n","# bais에 대한 weight \n","w1 = model_logR.layer.weight[0][0].item()\n","w2 = model_logR.layer.weight[0][1].item()\n","b = model_logR.layer.bias.item()\n","\n","print(w1, w2, b)\n","\n","xp = np.arange(-4, 4, 0.01).reshape(-1, 1)\n","yp = - w1 / w2 * xp - b / w2\n","\n","# 앞서 정의한 train_x,y를 plot하기 위해 pug로 만든 것을 cpu로 되돌림\n","train_X, train_y = train_X.cpu(), train_y.cpu()\n","\n","plt.figure(figsize = (10,8))\n","plt.plot(train_X[C1,0], train_X[C1,1], 'ro', alpha = 0.3, label='C1')\n","plt.plot(train_X[C0,0], train_X[C0,1], 'bo', alpha = 0.3, label='C0')\n","plt.plot(xp, yp, 'g', linewidth = 3, label = 'Logistic Regression')\n","plt.xlabel(r'$x_1$', fontsize = 15)\n","plt.ylabel(r'$x_2$', fontsize = 15)\n","plt.legend(loc = 1, fontsize = 12)\n","plt.axis('equal')\n","plt.ylim([-4,4])\n","plt.show()\n","\n","# 두 개의 다른 dataset을 분류하기 위한 logistic regression을 돌렸고 \n","# 학습된 결과로 관련된 weight, W1, W2, bias와 관련된 weight가 도출됨 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"udWtDT7wP1yK"},"source":["## MNIST_MLP"]},{"cell_type":"code","metadata":{"id":"yLNAIgaTP4vJ"},"source":["# MLIST를 통해서 MLP 구현 \n","# image data -> torch vision \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHX_T8JpP7pR"},"source":["BATCH_SIZE = 32\n","EPOCHS = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35_e-3LwQVcn"},"source":["# torch vision 밑에 dataset이 존재하는데 그 중 MLIST dataset을 불러옴   \n","\n","train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n","                               train = True,\n","                               download = True,\n","                               transform = transforms.ToTensor())\n","\n","test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n","                              train = False,\n","                              transform = transforms.ToTensor())\n","\n","# Define train_loader, Test_loader\n","# data를 loader 형태로 변환 \n","# test 는 shuffle하지 않고 그대로 사용 \n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size = BATCH_SIZE,\n","                          shuffle = True)\n","\n","test_loader = DataLoader(dataset=test_dataset,\n","                         batch_size = BATCH_SIZE,\n","                         shuffle = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HezHz8TMQo6l"},"source":["# size 파악 \n","# batch size를 32로 했기 때문에 하나의 mini batch가 32개의 image data로 구성되어 있음을 알 수 있음 \n","# 각각의 image는 28*28 형태 \n","# y_train는 image에 해당하는 label만 포함되어 있기 때문에 단순히 batch size만큼만 할당됨 \n","# FloatTensor / LongTensor \n","\n","for (X_train, y_train) in train_loader:\n","    print('X_train:', X_train.size(), 'type:', X_train.type())\n","    print('y_train:', y_train.size(), 'type:', y_train.type())\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOhsHOuhQrnH"},"source":["# input이 어떤 형태로 들어있는가? \n","\n","pltsize = 1\n","plt.figure(figsize=(10 * pltsize, pltsize))\n","for i in range(10):\n","    plt.subplot(1, 10, i + 1)\n","    plt.axis('off')\n","    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n","    plt.title('Class: ' + str(y_train[i].item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Evy-mJ30Q-QE"},"source":["# MLP 정의하는 단계 -> NeuralNet이라는 class로 정의  \n","\n","class NeuralNet(torch.nn.Module):\n","    # Define init and forward method\n","    # nn module에 있는 method를 상속받아 사용하기 위한 super \n","    # fully connected layer 총 3개를 정의 \n","    # 1) linear, input이 28*28 -> 512개 형태의 output 만듬\n","    # 2) 512개의 input을 받아서 256개의 output 도출\n","    # 3) 256개의 input을 받아서 10개의 output(숫자, 0~9)도출  \n","    def __init__(self):\n","        super(NeuralNet, self).__init__()\n","        self.fclayer1 = torch.nn.Linear(28 * 28, 512)\n","        self.fclayer2 = torch.nn.Linear(512, 256)\n","        self.fclayer3 = torch.nn.Linear(256, 10)\n","        # drop out probability 0.5, 각각의 sigmoid function 후에 적용됨 \n","        # 참고로 drop out을 적용할 때는 sigmoid보다 relu가 좋기 때문에 relu로 적용 \n","        self.dropout_prob = 0.5\n","        # batch normalization -> activation function 앞에 적용\n","        self.batch_norm1 = nn.BatchNorm1d(512)\n","        self.batch_norm2 = nn.BatchNorm1d(256)\n","\n","    # 28*28 input을, 2차원 data를 1차원 data로 변환하는 과정 (view)\n","    # fcl1과 연결 -> sigmoid function연결 (activation function)\n","    # 다시 fc2와 연결 -> 다시 sigmoide로 연결 \n","    # 다시 fc3와 연결 -> 마지막으로 최종적인 output이 10개가 나오게 하는 log_softmax function을 활용\n","    # x return \n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28)\n","        x = self.fclayer1(x)\n","        # 첫 번째 batch normalization \n","        x = self.batch_norm1(x)\n","        x = F.relu(x)\n","        # 첫 번째 sigmoid function 이후 dropout, input은 x\n","        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n","        x = self.fclayer2(x)\n","        # 두 번째 batch normalization \n","        x = self.batch_norm2(x)\n","        x = F.relu(x)\n","        # 두 번째 sigmoid function 이후 dropout \n","        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n","        x = self.fclayer3(x)\n","        x = F.log_softmax(x, dim = 1)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUdXJmHLSzWN"},"source":["# train, validation도 해야 하기 때문에 device형태로 gpu 사용 유무를 받음 \n","if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","else:\n","    DEVICE = torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubkjhq3iSJf4"},"source":["# optimizer와 loss 정의 \n","\n","# model instance화 \n","model = NeuralNet().to(DEVICE)\n","\n","# Define optimizer\n","# optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n","\n","# Define loss \n","criterion = nn.CrossEntropyLoss()\n","\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Do6IcrwS5nq"},"source":["# training \n","\n","def train(model, train_loader, optimizer, log_interval):\n","    model.train()\n","    for batch_idx, (image, label) in enumerate(train_loader):\n","        # image, label을 gpu를 쓰게 되면 쓰게끔 하는 명령어 \n","        image = image.to(DEVICE)\n","        label = label.to(DEVICE)\n","        # training with output, loss, zero_grad, backward, step\n","        output = model(image)\n","        loss = criterion(output, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","   \n","        if batch_idx % log_interval == 0:\n","            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n","                epoch, batch_idx * len(image), \n","                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n","                loss.item())) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUYu2Z1lTktu"},"source":["# evaluation \n","\n","def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    # 여기서 중요한 것은 torch가 weight를 update하기 위해서 gradient를 활용하지 못하게 정의해야 함 \n","    with torch.no_grad():\n","        for image, label in test_loader:\n","            image = image.to(DEVICE)\n","            label = label.to(DEVICE)\n","            # Evaluate with output, test_loss, prediction, correct\n","            # test_loss와 correct에 대해서 관련된 값들을 누적시킬 것임 \n","            # test_loss -> 마지막 test_loader의 lenght로 나누어 계산할 것임 -> 계속 누적하면 됨 \n","            # prediction -> 도출된 output에 대해서 (각각의 label에 대한 확률로 나올 것임) 그 중 가장 높은 확률을 prediction 결과값으로 도출\n","            # correct -> accuracy 계산을 위함, test_loader.length로 나누어줄 것이기 때문에 누적 형태로 계산 \n","            # 도출된 prediction이 원래 값과 동일한지 아닌지를 확인하는 단계가 됨\n","            output = model(image)\n","            test_loss += criterion(output, label).item()\n","            prediction = output.max(1, keepdim = True)[1]\n","            correct += prediction.eq(label.view_as(prediction)).sum().item()\n","    \n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v6OyZcbUQYX"},"source":["# 직접 train, evaluation 진행 \n","# epoch 10번 당 한 번의 evaluation\n","# drop_out, batch normalization, optimizer -> Adam 3개가지만 적용 -> 90% -> 97% 성능 상승 \n","# 단순한 fully connected layer -> 90%, Regularization -> 적절한 일반화 과정이 필요합니다! \n","\n","for epoch in range(1, EPOCHS + 1):\n","    train(model, train_loader, optimizer, log_interval = 200)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n","        epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLhdQWlqpsIO"},"source":[""],"execution_count":null,"outputs":[]}]}